/* tslint:disable */
/* eslint-disable */
/**
 * Proxy Smart
 * SMART on FHIR Proxy + Healthcare Administration API using Keycloak and Elysia
 *
 * The version of the OpenAPI document: 0.0.1-alpha.202510130118.b027dfe
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


import * as runtime from '../runtime';
import type {
  AiHealthErrorResponse,
  ChatRequest,
  ChatResponse,
  ErrorResponse,
  GetHealth422Response,
} from '../models/index';
import {
    AiHealthErrorResponseFromJSON,
    AiHealthErrorResponseToJSON,
    ChatRequestFromJSON,
    ChatRequestToJSON,
    ChatResponseFromJSON,
    ChatResponseToJSON,
    ErrorResponseFromJSON,
    ErrorResponseToJSON,
    GetHealth422ResponseFromJSON,
    GetHealth422ResponseToJSON,
} from '../models/index';

export interface PostAiChatRequest {
    chatRequest: ChatRequest;
}

export interface PostAiChatStreamRequest {
    chatRequest: ChatRequest;
}

/**
 * 
 */
export class AiApi extends runtime.BaseAPI {

    /**
     * Returns health status including OpenAI availability and backend authentication status.
     * Get AI assistant health status
     */
    async getAiHealthRaw(initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<runtime.ApiResponse<{ [key: string]: any; }>> {
        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};


        let urlPath = `/ai/health`;

        const response = await this.request({
            path: urlPath,
            method: 'GET',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse<any>(response);
    }

    /**
     * Returns health status including OpenAI availability and backend authentication status.
     * Get AI assistant health status
     */
    async getAiHealth(initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<{ [key: string]: any; }> {
        const response = await this.getAiHealthRaw(initOverrides);
        return await response.value();
    }

    /**
     * Returns 200 when the AI assistant backend is reachable.
     * Check AI assistant availability
     */
    async headAiChatRaw(initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<runtime.ApiResponse<void>> {
        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};


        let urlPath = `/ai/chat`;

        const response = await this.request({
            path: urlPath,
            method: 'HEAD',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.VoidApiResponse(response);
    }

    /**
     * Returns 200 when the AI assistant backend is reachable.
     * Check AI assistant availability
     */
    async headAiChat(initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<void> {
        await this.headAiChatRaw(initOverrides);
    }

    /**
     * Forwards chat prompts to the MCP AI assistant server and returns enriched responses.
     * Proxy AI assistant chat request
     */
    async postAiChatRaw(requestParameters: PostAiChatRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<runtime.ApiResponse<ChatResponse>> {
        if (requestParameters['chatRequest'] == null) {
            throw new runtime.RequiredError(
                'chatRequest',
                'Required parameter "chatRequest" was null or undefined when calling postAiChat().'
            );
        }

        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';


        let urlPath = `/ai/chat`;

        const response = await this.request({
            path: urlPath,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: ChatRequestToJSON(requestParameters['chatRequest']),
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => ChatResponseFromJSON(jsonValue));
    }

    /**
     * Forwards chat prompts to the MCP AI assistant server and returns enriched responses.
     * Proxy AI assistant chat request
     */
    async postAiChat(requestParameters: PostAiChatRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<ChatResponse> {
        const response = await this.postAiChatRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * Forwards chat prompts to the MCP AI assistant server and streams the response using Server-Sent Events (SSE).  **Stream Event Types:** The response is a stream of `data: {...}` events, where each event is a JSON object matching the StreamChunk schema: - `{type: \'sources\', sources: [...], mode?: string, confidence?: number}` - Relevant document sources - `{type: \'content\', content: string}` - Response text chunk - `{type: \'reasoning\', content: string}` - AI reasoning/thinking process - `{type: \'reasoning_done\'}` - End of reasoning phase - `{type: \'function_calling\', name: string}` - Function being called - `{type: \'done\', mode?: string, confidence?: number}` - End of stream - `{type: \'error\', error: string}` - Error occurred  See the `StreamChunk` schema component for full type definitions.
     * Proxy AI assistant streaming chat request
     */
    async postAiChatStreamRaw(requestParameters: PostAiChatStreamRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<runtime.ApiResponse<{ [key: string]: any; }>> {
        if (requestParameters['chatRequest'] == null) {
            throw new runtime.RequiredError(
                'chatRequest',
                'Required parameter "chatRequest" was null or undefined when calling postAiChatStream().'
            );
        }

        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';


        let urlPath = `/ai/chat/stream`;

        const response = await this.request({
            path: urlPath,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: ChatRequestToJSON(requestParameters['chatRequest']),
        }, initOverrides);

        return new runtime.JSONApiResponse<any>(response);
    }

    /**
     * Forwards chat prompts to the MCP AI assistant server and streams the response using Server-Sent Events (SSE).  **Stream Event Types:** The response is a stream of `data: {...}` events, where each event is a JSON object matching the StreamChunk schema: - `{type: \'sources\', sources: [...], mode?: string, confidence?: number}` - Relevant document sources - `{type: \'content\', content: string}` - Response text chunk - `{type: \'reasoning\', content: string}` - AI reasoning/thinking process - `{type: \'reasoning_done\'}` - End of reasoning phase - `{type: \'function_calling\', name: string}` - Function being called - `{type: \'done\', mode?: string, confidence?: number}` - End of stream - `{type: \'error\', error: string}` - Error occurred  See the `StreamChunk` schema component for full type definitions.
     * Proxy AI assistant streaming chat request
     */
    async postAiChatStream(requestParameters: PostAiChatStreamRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<{ [key: string]: any; }> {
        const response = await this.postAiChatStreamRaw(requestParameters, initOverrides);
        return await response.value();
    }

}
